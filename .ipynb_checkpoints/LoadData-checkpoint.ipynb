{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we can see/access data\n",
    "!ls ~/CORD19v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/home/eugene_chuvyrov/CORD19v3/'\n",
    "metadata_path = f'{root_path}/metadata.csv'\n",
    "meta_df = pd.read_csv(metadata_path, dtype={\n",
    "    'pubmed_id': str,\n",
    "    'Microsoft Academic Paper ID': str, \n",
    "    'doi': str\n",
    "})\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
    "len(all_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class to hold info about each paper\n",
    "class FileReader:\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path) as file:\n",
    "            content = json.load(file)\n",
    "            self.paper_id = content['paper_id']\n",
    "            self.abstract = []\n",
    "            self.body_text = []\n",
    "            self.references = []\n",
    "            \n",
    "            # Abstract\n",
    "            if 'abstract' in content:\n",
    "                for entry in content['abstract']:\n",
    "                    self.abstract.append(entry['text'])\n",
    "            if 'Abstract' in  content:\n",
    "                for entry in content['Abstract']:\n",
    "                    self.abstract.append(entry['text'])\n",
    "            \n",
    "            # Body text\n",
    "            for entry in content['body_text']:\n",
    "                self.body_text.append('--BEGIN PARAGRAPH---')\n",
    "                self.body_text.append(entry['text'])\n",
    "                self.body_text.append('--END PARAGRAPH---')\n",
    "            #References\n",
    "            for key, reference in content['bib_entries'].items():\n",
    "                self.references.append(reference['title'])\n",
    "                \n",
    "            self.abstract = '\\n'.join(self.abstract)\n",
    "            self.body_text = '\\n'.join(self.body_text)\n",
    "    def __repr__(self):\n",
    "        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n",
    "    \n",
    "first_row = FileReader(all_json[0])\n",
    "print(first_row.body_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breaks(content, length):\n",
    "    data = \"\"\n",
    "    words = content.split(' ')\n",
    "    total_chars = 0\n",
    "\n",
    "    # add break every length characters\n",
    "    for i in range(len(words)):\n",
    "        total_chars += len(words[i])\n",
    "        if total_chars > length:\n",
    "            data = data + \"<br>\" + words[i]\n",
    "            total_chars = 0\n",
    "        else:\n",
    "            data = data + \" \" + words[i]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all papers and populate FileReader objects\n",
    "dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'references': [], 'title': [], 'journal': [], 'abstract_summary': []}\n",
    "for idx, entry in enumerate(all_json):\n",
    "    if idx % (len(all_json) // 10) == 0:\n",
    "        print(f'Processing index: {idx} of {len(all_json)}')\n",
    "    content = FileReader(entry)\n",
    "\n",
    "    # print('processing file ' + entry)\n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    # no metadata, skip this paper\n",
    "    if len(meta_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    dict_['paper_id'].append(content.paper_id)\n",
    "    dict_['abstract'].append(content.abstract)\n",
    "    dict_['body_text'].append(content.body_text)\n",
    "    dict_['references'].append(content.references)\n",
    "    \n",
    "    # also create a column for the summary of abstract to be used in a plot\n",
    "    if len(content.abstract) == 0: \n",
    "        # no abstract provided\n",
    "        dict_['abstract_summary'].append(\"Not provided.\")\n",
    "    elif len(content.abstract.split(' ')) > 100:\n",
    "        # abstract provided is too long for plot, take first 300 words append with ...\n",
    "        info = content.abstract.split(' ')[:100]\n",
    "        summary = get_breaks(' '.join(info), 40)\n",
    "        dict_['abstract_summary'].append(summary + \"...\")\n",
    "    else:\n",
    "        # abstract is short enough\n",
    "        summary = get_breaks(content.abstract, 40)\n",
    "        dict_['abstract_summary'].append(summary)\n",
    "        \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    \n",
    "    try:\n",
    "        # if more than one author\n",
    "        authors = meta_data['authors'].values[0].split(';')\n",
    "        if len(authors) > 2:\n",
    "            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n",
    "            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n",
    "        else:\n",
    "            # authors will fit in plot\n",
    "            dict_['authors'].append(\". \".join(authors))\n",
    "    except Exception as e:\n",
    "        # if only one author - or Null valie\n",
    "        dict_['authors'].append(meta_data['authors'].values[0])\n",
    "    \n",
    "    # add the title information\n",
    "    title = meta_data['title'].values[0]\n",
    "    dict_['title'].append(title)\n",
    "    \n",
    "    # add the journal information\n",
    "    dict_['journal'].append(meta_data['journal'].values[0])\n",
    "    \n",
    "df_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'references', 'abstract_summary'])\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))\n",
    "df_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['abstract'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates (based on identical abstract)\n",
    "df_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\n",
    "df_covid['abstract'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.dropna(inplace=True)\n",
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take butcher's knife and kill all special characters from our docs\n",
    "#  this is brutal and leads to a massive information loss in our scientific corpus of data\n",
    "#  but this will do for initial modeling\n",
    "import re\n",
    "\n",
    "df_covid['body_text'] = df_covid['body_text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# everything will be lowered (lowered-case that is)\n",
    "def lower_case(input_str):\n",
    "    input_str = input_str.lower()\n",
    "    return input_str\n",
    "\n",
    "df_covid['body_text'] = df_covid['body_text'].apply(lambda x: lower_case(x))\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(lambda x: lower_case(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since classification of data for training was done manually (i.e., by reading each paragraph and selecting an appropriate category)\n",
    "#  this is a feeble attempt at prioritizing which articles to read first\n",
    "#  here, the ones I'll read first will be the ones that are most frequently referenced by other papers in the corpus\n",
    "#  ala Google PageRank, much simplified\n",
    "#  obvious weakness of this approach is the prioritization of older literature, which may contain outdated facts\n",
    "def create_article_refs_counts(df_covid19):\n",
    "    dict_all_refs = {}\n",
    "    \n",
    "    # scan through all the articles in the dataframe\n",
    "    #  look at their references\n",
    "    #  add +1 if the reference matches the one we've already seen before\n",
    "    for idx in df_covid19.index:\n",
    "        titles = df_covid19['references'][idx]\n",
    "        \n",
    "        for title in titles:\n",
    "            if dict_all_refs.get(title) != None:\n",
    "                dict_all_refs[title] = dict_all_refs[title] + 1\n",
    "            else:\n",
    "                dict_all_refs[title] = 1\n",
    "                \n",
    "    return dict_all_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def is_date(string, fuzzy=False):\n",
    "    \"\"\"\n",
    "    Return whether the string can be parsed as a date in our expected format\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(string, str):\n",
    "        return False\n",
    "    \n",
    "    try: \n",
    "        meta_published_date = datetime.strptime(string, '%Y-%m-%d')\n",
    "        return True\n",
    "\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def get_top100_referenced_articles_metadata():\n",
    "    dict_top100_ = {'article_title': [], 'meta_link': [], 'frequency': []}\n",
    "    # now find most frequently referenced articles\n",
    "    article_references = create_article_refs_counts(df_covid)\n",
    "    sorted_article_refs = sorted(article_references.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    for index, row in meta_df.iterrows():\n",
    "        meta_title = row['title']\n",
    "        meta_link = row['doi']\n",
    "\n",
    "        # some unexpected data types handling\n",
    "        if isinstance(meta_title, str):\n",
    "            meta_title = ''.join(meta_title.split()).lower()\n",
    "\n",
    "        # more unexpected data types handling\n",
    "        if isinstance(meta_link, str):\n",
    "            if not meta_link.startswith('http://doi.org/'):\n",
    "                meta_link = 'http://doi.org/' + meta_link\n",
    "\n",
    "        for article_index in range(30000):\n",
    "            # make sure the referenced article exists in our metadata\n",
    "            frequency = sorted_article_refs[article_index][1]\n",
    "            article_title = sorted_article_refs[article_index][0]\n",
    "            frequent_title = ''.join(article_title.split()).lower()\n",
    "\n",
    "            if(frequent_title == meta_title):\n",
    "                dict_top100_['article_title'].append(article_title)\n",
    "                dict_top100_['meta_link'].append(meta_link)\n",
    "                dict_top100_['frequency'].append(frequency)\n",
    "                \n",
    "    return dict_top100_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100md = get_top100_referenced_articles_metadata()\n",
    "df_top100 = pd.DataFrame(top100md, columns=['article_title', 'meta_link', 'frequency'])\n",
    "\n",
    "df_top100 = df_top100.sort_values(by=['frequency'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100md.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now produce a data frame listing the articles, one paragraph per row\n",
    "#  the unit of classification will be a paragraph of text, not an article\n",
    "#  something that may generalize well (or not)\n",
    "def get_training_set():\n",
    "    dict_trainingset_ = {'paper_id': [], 'article_title': [], 'meta_link': [], 'abstract': [], 'paragraph_text': [], 'frequency': [], 'authors': [], 'references': [], 'journal': [], 'abstract_summary': []}\n",
    "\n",
    "    for index, row in df_covid.iterrows():\n",
    "        covid_title = row['title']\n",
    "        \n",
    "        # some unexpected data types handling\n",
    "        if isinstance(covid_title, str):\n",
    "            covid_title = ''.join(covid_title.split()).lower()\n",
    "\n",
    "        article_title = row['article_title']\n",
    "        meta_link = row['meta_link']\n",
    "        frequency = row['frequency']\n",
    "        lower_title = ''.join(article_title.split()).lower()\n",
    "\n",
    "        article_text = row['body_text']\n",
    "        paragraphs = article_text.split('begin paragraph')\n",
    "\n",
    "        for paragraph in paragraphs:                    \n",
    "            dict_trainingset_['article_title'].append(article_title)\n",
    "            dict_trainingset_['meta_link'].append(meta_link)\n",
    "            dict_trainingset_['frequency'].append(frequency)\n",
    "\n",
    "            dict_trainingset_['paper_id'].append(row['paper_id'])\n",
    "            dict_trainingset_['abstract'].append(row['abstract'])\n",
    "            dict_trainingset_['paragraph_text'].append(paragraph.strip())\n",
    "            dict_trainingset_['authors'].append(row['authors'])\n",
    "            dict_trainingset_['references'].append(row['references'])\n",
    "            dict_trainingset_['journal'].append(row['journal'])\n",
    "            dict_trainingset_['abstract_summary'].append(row['abstract_summary'])\n",
    "\n",
    "    return dict_trainingset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = get_training_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(training_set['article_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.keys()\n",
    "print(len(training_set['abstract']), len(training_set['paragraph_text']), len(training_set['frequency']), len(training_set['authors']), len(training_set['references']), len(training_set['journal']), len(training_set['abstract_summary']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_set = pd.DataFrame(training_set, columns=['paper_id', 'article_title', 'meta_link', 'abstract', 'paragraph_text', 'frequency', 'authors', 'references', 'journal', 'abstract_summary'])\n",
    "final_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_set.to_csv('~/CORD19v3/test_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
